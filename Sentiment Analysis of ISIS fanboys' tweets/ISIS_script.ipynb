{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from wordcloud import WordCloud\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import fasttext\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the csv file\n",
    "\n",
    "df_tweets = pd.read_csv(\"tweets_train.csv\", parse_dates=True,na_values=' ',encoding=\"ISO-8859-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing basic preprocessing operations- setting index, replacing null values with empty strings, removing duplicate entries and\n",
    "#converting text into lower case\n",
    "\n",
    "def preprocessing(tweet):\n",
    "    tweet.set_index(['time','username'], inplace = True)\n",
    "    tweet.drop(['name'],axis = 1, inplace = True)\n",
    "    tweet['description'] = tweet['description'].fillna('')\n",
    "    tweet['location'] = tweet['location'].fillna('')\n",
    "    tweet=tweet.drop_duplicates()\n",
    "    tweet['description'] = tweet['description'].str.lower()\n",
    "    tweet['tweets'] = tweet['tweets'].str.lower()\n",
    "    return tweet\n",
    "\n",
    "df_tweets =preprocessing(df_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 17410 entries, ('04-01-2016 01:26', 'squadsquaaaaad') to ('3/31/2016 20:25', 'wayyf44rer')\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   description     17410 non-null  object\n",
      " 1   location        17410 non-null  object\n",
      " 2   followers       17410 non-null  int64 \n",
      " 3   numberstatuses  17410 non-null  int64 \n",
      " 4   tweets          17410 non-null  object\n",
      " 5   label           17410 non-null  int64 \n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_tweets.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['description', 'location', 'followers', 'numberstatuses', 'tweets',\n",
       "       'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uncle_SamCoco                                                                                                                                                                                                                                                                                                       0.090752\n",
       "RamiAlLolah                                                                                                                                                                                                                                                                                                         0.084721\n",
       "warrnews                                                                                                                                                                                                                                                                                                            0.068409\n",
       "WarReporter1                                                                                                                                                                                                                                                                                                        0.062895\n",
       "mobi_ayubi                                                                                                                                                                                                                                                                                                          0.060655\n",
       "                                                                                                                                                                                                                                                                                                                      ...   \n",
       "Abdul__05                                                                                                                                                                                                                                                                                                           0.000287\n",
       "abuayisha102                                                                                                                                                                                                                                                                                                        0.000287\n",
       "newerajihadi61                                                                                                                                                                                                                                                                                                      0.000230\n",
       "fahadslay614                                                                                                                                                                                                                                                                                                        0.000230\n",
       "````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````    0.000057\n",
       "Name: username, Length: 112, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the number of unique users\n",
    "\n",
    "df_tweets.index.get_level_values(1).value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations and special characters from 'tweets' column  using regular expression operations and split tweets into a list of words\n",
    "\n",
    "def remove_punctuation(tweet):   \n",
    "    tweet_tokens = [re.sub (r'http\\S+', 'url', x) for x in tweet]   \n",
    "    tweet_tokens = [re.sub(r'(\\s)#\\w+', r'\\1', x) for x in tweet_tokens] \n",
    "    tweet_tokens = [re.sub(r'(\\s)@\\w+', r'\\1', x) for x in tweet_tokens]\n",
    "    tweet_tokens = [re.sub (r'[\\W_]+',' ',x) for x in tweet_tokens]\n",
    "    tweet_tokens_split =[x.split(\" \") for x in tweet_tokens]\n",
    "    return tweet_tokens_split\n",
    "\n",
    "\n",
    "df_tweets['tweet_tokens_split']=remove_punctuation(df_tweets['tweets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing single character words from tweet\n",
    "\n",
    "def singlecharword_remove(tweet):\n",
    "    tweet_nosinglecharword=[]\n",
    "    for word_list in tweet:\n",
    "        for word in word_list[:]:\n",
    "            if len(word) <= 2:\n",
    "                word_list.remove(word)\n",
    "        tweet_nosinglecharword.append(word_list)\n",
    "    return tweet_nosinglecharword\n",
    "\n",
    "tweet_nosinglecharword=singlecharword_remove( df_tweets['tweet_tokens_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually creating a dictionary of words and replacing the words in the 'tweets' column with their root\n",
    "\n",
    "def replace_withrootword_manual(tweet):\n",
    "    list_words= ['turkey','islamicstate','syria','aleppo','warreporter1','russia', 'assad', 'iraq','url','the','abu','isis','attack','muslim','islam']\n",
    "    dict_repetition=defaultdict(list)\n",
    "    for word_list in tweet:\n",
    "        for i, v in enumerate(word_list):\n",
    "            for item in list_words:\n",
    "                if item in v:          \n",
    "                    word_list[i] = v.replace(v, item)\n",
    "    return tweet\n",
    " \n",
    "\n",
    "tweet_replacedwithroot_manual=replace_withrootword_manual(tweet_nosinglecharword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the list of words in tweet column\n",
    "\n",
    "def join_tweets(tweet):\n",
    "    tweet_tokens_joined =[' '.join(words) for words in tweet]\n",
    "    return tweet_tokens_joined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the 'tweet_tokens_joined' column to a txt file\n",
    "\n",
    "tweet_tokens_joined_rootmanual=join_tweets(tweet_replacedwithroot_manual)\n",
    "df_tweets['tweet_tokens_joined'] = tweet_tokens_joined_rootmanual\n",
    "\n",
    "df_tweets['tweet_tokens_joined'].to_csv(r'tweets_clean.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised('tweets_clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_withrootword_fasttext(tweet):   \n",
    "    dict_repetition=defaultdict(list)\n",
    "    threshold_fasttext=.995\n",
    "    for word_list in tweet:           \n",
    "            for word in set(word_list):\n",
    "                for x in model.get_nearest_neighbors(word):\n",
    "                    if x[0]> threshold_fasttext:                                      \n",
    "                        dict_repetition[word].append(x[1])\n",
    "    \n",
    "    for word_list in tweet:\n",
    "        for i,word in enumerate(word_list):\n",
    "            for key,value in dict_repetition.items():\n",
    "                if word in value:\n",
    "                    word_list[i]=key  \n",
    "    return tweet\n",
    "tweet_replacedwithroot_fasttext = replace_withrootword_fasttext(tweet_replacedwithroot_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the low IDF(Inverse Document Frequency ) terms to add them to the list of stop words\n",
    "\n",
    "def tf_idf(tweet):\n",
    "    tf = defaultdict(int)\n",
    "    for word_list in tweet:\n",
    "        for word in set(word_list):       \n",
    "             tf[word]+=1             \n",
    "    idf = defaultdict(int)\n",
    "    count_tweets= df_tweets.tweets.count()\n",
    "    for key, value in tf.items():\n",
    "        idf[key]=math.log(count_tweets/(value + 1))\n",
    "   \n",
    "    return sorted(idf.items(), key=lambda x: x[1],reverse=False)\n",
    "\n",
    "#tf_idf(tweet_replacedwithroot_fasttext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of stop words has been created manually from the low IDF(Inverse Document Frequency ) terms\n",
    "\n",
    "stop_words_manual=['rt','abu', 'url','amp','the', 'in', 'of', 'to', 'and', 'is', 'on', 'by', 'for', 'with', '&amp;', 'from', 'are', 'you', 'they', 'that', 'this', 'it', 'us', 'was', 'have', 'their', 'will', 'an', 'who', 'be', 'as', 'after', 'at', 'he', 'al', 'but', 'its', 'has', 'his', 'one', 'were', 'if', 'all', 'today', 'them', 'we', 'city', 'people', 'about', 'your', 'now', 'when', 'ypg', 'what', 'more', 'or', 'new', 'over', 'like', 'just', 'so', 'can', 'de', 'west', 'north', 'saa','how', 'only', 'do', 'claims', 'reports', 'those', 'captured', 'fsa', 'than', 'my', 'back', 'up', 'la', 'between', 'group', 'our', 'there', 'out', 'two', 'area', 'while', 'via', 'which', 'know', 'because', 'been','see', 'le', 'coalition', 'me', 'village', 'south', 'northern', 'news', 'many', 'him', 'east', 'time', 'being', 'some', 'still', 'these', 'think', 'support', 'then', 'may', 'a', 'why', 'make', 'air','take', 'also', 'want', 'huge', 'says', 'even', 'un', 'under', 'during', 'other', 'very', 'homs','say', 'les', 'targeted', 'help', 'into','days', 'reportedly', 'eastern', 'any','said','please', 'des', 'where', 'would', 'first', 'yesterday', 'get', 'ied', 'heavy', 'report', 'day', 'security', 'same', 'twitter','im', 'ahrar', 'least', 'httâ€¦', 'years', 'cest', 'deir','je', 'vso','rt', 'url', 'the', 'in', 'of', 'to', 'and', 'is', '', 'on', 'by', 'for', '&amp;', 'with', 'from', 'you', 'are', 'they', 'this', 'that', 'it', 'was', 'have', 'who', 'after', 'their', 'will', 'as', 'an', 'be', 'he', 'near', 'its', 'at', 'but', 'them', 'has', \n",
    "'if','plz', 'were', 'should','did','does','since','amp', 'let', 'his', 'al', 'english', 'translation', 'today', 'one', 'when', 'people', 'or', 'city', 'what', 'we', 'now', 'so', 'all', 'more', 'only','about', 'your', 'ypg', 'de', 'new', 'reports', 'just', 'over', 'saa', 'like', 'my', 'do', 'i', 'can', 'area', 'there','up', 'between', 'how', 'our', 'than', 'claims', 'group', 'out', 'may', 'la', 'huge', 'those', 'me', 'some', 'le', 'time', 'see', 'been', 'know', 'make', 'many', 'him', 'still', 'which', 'why', 'clashes', 'says', 'two', 'south', 'because', 'via', 'these', 'support', 'being', 'then', 'un', 'other', 'said', 'control', 'back', 'under', 'very', 'where', 'while', 'any', 'say', 'even', 'air', 'lol', 'during', 'eastern', 'get', 'days', 'want', 'please', 'also', 'most','first', 'im', 'into', 'another', 'les', 'would', 'positions', 'reportedly', '&gt']\n",
    "\n",
    "stop_words_manual = list(set(stop_words_manual))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the new 'tweet_tokens_joined' column to a txt file\n",
    "tweet_tokens_joined_rootfasttext=join_tweets(tweet_replacedwithroot_fasttext)\n",
    "df_tweets['tweet_tokens_joined'] = tweet_tokens_joined_rootfasttext\n",
    "df_tweets['tweet_tokens_joined'].to_csv(r'tweets_clean.txt', header=None, index=None, sep=' ', mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using fasttext to find stopwords\n",
    "\n",
    "model = fasttext.train_unsupervised('tweets_clean.txt')\n",
    "\n",
    "def stopword_fasttext(stopwords):\n",
    "    threshold_fasttext = 0.4\n",
    "    list_of_words=[]\n",
    "    for word in stopwords:                 \n",
    "            for x in model.get_nearest_neighbors(word):\n",
    "                if x[0]> threshold_fasttext:\n",
    "                        list_of_words.append(x[1])                \n",
    "    return list_of_words\n",
    "        \n",
    "stop_words_fasttext  = stopword_fasttext(stop_words_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words= stop_words_manual+stop_words_fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords from the list of words in 'tweets' column\n",
    "\n",
    "def remove_stopwords(stopword,tweet):     \n",
    "    for word_list in tweet:   \n",
    "        for word in stopword:\n",
    "             if word in word_list[:]:          \n",
    "                word_list.remove(word)        \n",
    "    return tweet\n",
    "tweet_nostopwords=remove_stopwords(stop_words,tweet_replacedwithroot_fasttext)\n",
    "df_tweets['tweet_nostopwords'] = tweet_nostopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens_joined=join_tweets(tweet_nostopwords)\n",
    "df_tweets['tweet_tokens_joined'] = tweet_tokens_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating word cloud to identify the key terms in  negative tweets\n",
    "neagtive_tweets= df_tweets['tweet_nostopwords'][df_tweets['label']==-1]\n",
    "tweet_joined_wordcloud=' '.join(join_tweets(neagtive_tweets))\n",
    "#print(tweet_joined_wordcloud)\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='black',                 \n",
    "                min_font_size = 10).generate(str(tweet_joined_wordcloud))   \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0)   \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating word cloud to identify the key terms in  positive tweets\n",
    "positive_tweets= df_tweets['tweet_nostopwords'][df_tweets['label']==1]\n",
    "tweet_joined_wordcloud=' '.join(join_tweets(positive_tweets))\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='black',                 \n",
    "                min_font_size = 10).generate(str(tweet_joined_wordcloud))   \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0)   \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a bar chart, visualizing the distribution of number of tweets by various users\n",
    "\n",
    "df_tweet_resetindex=df_tweets.reset_index(level='username')\n",
    "dict_username=df_tweet_resetindex['username'].value_counts()[df_tweet_resetindex['username'].value_counts()>200].to_dict()\n",
    "fig= plt.figure(figsize=(15,10))\n",
    "df_username = pd.Series(dict_username)\n",
    "plt.bar(range(len(df_username)), df_username.values)\n",
    "plt.xticks(range(len(df_username)), df_username.index.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Name of user') \n",
    "plt.ylabel('Number of tweets') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv('tweet_preprocessed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using word2vec model to find similar words \n",
    "# attempt to build neural network model\n",
    "\n",
    "w2v_model = Word2Vec(min_count=5,\n",
    "                     window=4,\n",
    "                     size=100,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20)\n",
    "w2v_model.build_vocab(tweet_nostopwords, progress_per=10000)\n",
    "w2v_model.train(tweet_nostopwords, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.wv.most_similar(topn=10,positive='militant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using fasttext model to find similar words\n",
    "# attempt to build neural network model\n",
    "\n",
    "model = fasttext.train_unsupervised('tweets_clean.txt')\n",
    "model.get_nearest_neighbors('militant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a Naive bayes classification model using nltk library\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=df_tweets[\"tweet_nostopwords\"]\n",
    "y=df_tweets[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=42, stratify=y)\n",
    "\n",
    "X_train = X_train.tolist() \n",
    "X_test = X_test.tolist() \n",
    "y_train = y_train.tolist() \n",
    "y_test = y_test.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processed_tweets(X,y):\n",
    "    tweets=[]\n",
    "    for tweet, label in zip( X,y):\n",
    "        tweets.append((tweet,label))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocabulary(train_data):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, sentiment) in train_data:\n",
    "        all_words.extend(words)\n",
    "    wordlist = nltk.FreqDist(all_words)\n",
    "    word_features = wordlist.keys()\n",
    "    \n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:        \n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = buildVocabulary(processed_tweets(X_train, y_train))\n",
    "trainingFeatures = nltk.classify.apply_features(extract_features, processed_tweets(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( NBayesClassifier , open( \"tweet_pickled.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBayesClassifier = pickle.load(open( \"tweet_pickled.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in processed_tweets(X_test,y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8431935669155658\n",
      "F1 score: 0.7980769230769231\n",
      "Precision: 0.8155706727135299\n",
      "roc-auc score: 0.8325913559316775\n",
      "Recall 0.781317885590152\n",
      "Confusion_matrix: \n",
      " [[1857  244]\n",
      " [ 302 1079]]\n"
     ]
    }
   ],
   "source": [
    "#checking the model performance\n",
    "\n",
    "def model_performance():      \n",
    "    print(\"Accuracy:\",sklearn.metrics.accuracy_score(y_test, NBResultLabels))    \n",
    "    print(\"F1 score:\",sklearn.metrics.f1_score(y_test, NBResultLabels))\n",
    "    print(\"Precision:\",sklearn.metrics.precision_score(y_test, NBResultLabels))     \n",
    "    print(\"roc-auc score:\",sklearn.metrics.roc_auc_score(y_test, NBResultLabels))\n",
    "    print(\"Recall\",sklearn.metrics.recall_score(y_test, NBResultLabels))\n",
    "    print(\"Confusion_matrix: \\n\",sklearn.metrics.confusion_matrix(y_test, NBResultLabels))\n",
    "model_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a Naive bayes clssification model using scikit learn as nltk doesn't  support cross validation\n",
    "\n",
    "def make_xy(tweet, vectorizer=None):    \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(tweet.tweet_tokens_joined)\n",
    "    X = X.tocsc()  \n",
    "    y = tweet.label.values.astype(np.int)\n",
    "    return X, y\n",
    "X, y = make_xy(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.79006318 0.79063756 0.80068926 0.78173464 0.78805284]\n",
      "F1 score: [0.70583501 0.70377895 0.72503962 0.69624301 0.71724138]\n",
      "roc-auc score: [0.89688266 0.89288751 0.90438467 0.8908467  0.89116779]\n",
      "precision: [0.79438406 0.78158845 0.78272027 0.78966455 0.7832636 ]\n",
      "recall: [0.63504707 0.64005913 0.67527675 0.62258756 0.6614841 ]\n",
      "Confusion Matrix:\n",
      " [[9293 1214]\n",
      " [2438 4465]]\n"
     ]
    }
   ],
   "source": [
    "#Model performnce after cross validation\n",
    "\n",
    "def model_performance_crossval():\n",
    "    clf = MultinomialNB(alpha=.1)\n",
    "    k_fold = KFold( n_splits=5, shuffle=True, random_state=0)\n",
    "    print(\"Accuracy:\", cross_val_score(clf, X, y, cv=k_fold, n_jobs=1,scoring='accuracy'))\n",
    "    print(\"F1 score:\",cross_val_score(clf, X, y, n_jobs=1, cv=k_fold,scoring='f1'))\n",
    "    print(\"roc-auc score:\",cross_val_score(clf, X, y, cv=k_fold, n_jobs=1,scoring='roc_auc'))\n",
    "    print(\"precision:\",cross_val_score(clf, X, y, cv=k_fold, n_jobs=1,scoring='precision'))\n",
    "    print(\"recall:\",cross_val_score(clf, X, y, cv=k_fold, n_jobs=1,scoring='recall'))\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=k_fold)\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "model_performance_crossval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing empty rows and rows with a single word \n",
    "def emptyrows_remove(tweet, nostopwords):    \n",
    "    min_words = 1\n",
    "    tweet = tweet[nostopwords.apply(lambda x: True  if len(x)>min_words else False)]      \n",
    "    return tweet\n",
    "\n",
    "df_tweets = emptyrows_remove(df_tweets, df_tweets['tweet_nostopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.82063492 0.82825397 0.82343601 0.8316926  0.82661162]\n",
      "F1 score: [0.72692122 0.73596877 0.73773585 0.75       0.73209028]\n",
      "roc-auc score: [0.89744048 0.9056932  0.89795961 0.91014723 0.89921034]\n",
      "precision: [0.77366255 0.75475475 0.77272727 0.79104478 0.78526316]\n",
      "recall: [0.68550593 0.71809524 0.70577617 0.71300448 0.68566176]\n",
      "Confusion Matrix:\n",
      " [[9180 1109]\n",
      " [1629 3829]]\n"
     ]
    }
   ],
   "source": [
    "# Checking performance after removing empty rows and rows with a single word \n",
    "\n",
    "X, y = make_xy(df_tweets)\n",
    "model_performance_crossval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(y,columns=['Output'])\n",
    "df.to_excel(excel_writer = \"test.xlsx\",encoding=\"ISO-8859-1\")\n",
    "df_tweets['Prediction'] =df['Output']\n",
    "#C:\\\\Users\\\\USER\\\\Documents\\\\Geetha\\\\Data Science\\\\Github\\\\Springboard\\\\Capstone Poject 1\\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
